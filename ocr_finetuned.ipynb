{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSm59aQ__Uei",
        "outputId": "1f2cc128-1dc3-40e3-d39f-48696d0791bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import os\n",
        "import io\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "\n",
        "Base= '/content/drive/MyDrive/Fum/term9/Project_phase2'\n",
        "lable= os.path.join(Base,'lable.csv')\n",
        "trainCSV=os.path.join(Base ,'train.csv')\n",
        "validationCSV=os.path.join(Base ,'validation.csv')\n",
        "testCSV= os.path.join(Base,'test.csv')\n",
        "df=pd.read_csv(lable)\n",
        "train_df,temp_df=train_test_split(df,test_size=0.3,random_state=42)\n",
        "val_df,test_df= train_test_split(temp_df,test_size=0.5 ,random_state=42)\n",
        "train_df.to_csv(trainCSV,index=False)\n",
        "val_df.to_csv(validationCSV,index=False)\n",
        "test_df.to_csv(testCSV ,index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XctEJ-GH_8RO"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/Fum/term9/Project_phase2/* /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T9yqkzYCH0I",
        "outputId": "6d1e6b98-ac5b-4282-add1-6b6df5d678c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh4mPHyQDdzK"
      },
      "outputs": [],
      "source": [
        "!rm -rf ~/.cache/huggingface/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9FayZMAGByE",
        "outputId": "34a76541-a000-44ba-bb8c-b34253b3b719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.6.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub pyarrow pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btu3eaqGJTZd",
        "outputId": "4f38c5ef-8334-476a-d6b9-75c6f4a9e495"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'persian-license-plate-v1'...\n",
            "remote: Enumerating objects: 95, done.\u001b[K\n",
            "remote: Total 95 (delta 0), reused 0 (delta 0), pack-reused 95 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (95/95), 138.43 KiB | 833.00 KiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/datasets/hezarai/persian-license-plate-v1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmjgWP0SLQGV",
        "outputId": "e11e0757-519b-44aa-92b6-03b565118555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test-00000-of-00001.parquet   validation-00000-of-00001.parquet\n",
            "train-00000-of-00001.parquet\n"
          ]
        }
      ],
      "source": [
        "!ls ./persian-license-plate-v1/data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3ZtPLOfLpPX",
        "outputId": "342b7ad1-0742-443e-c03f-8ef93a47c468"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['image_path', 'label'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df= pd.read_parquet('./persian-license-plate-v1/data/train-00000-of-00001.parquet')\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from PIL import Image\n",
        "import io\n",
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_transform=transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((64,128)),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2),\n",
        "    transforms.GaussianBlur(kernel_size=5,sigma=0.5),\n",
        "    transforms.RandomAffine(degrees=0,shear=5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5],std=[0.5])\n",
        "])\n",
        "val_transform=transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((64,128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5],std=[0.5])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQDSz3DYKNc9",
        "outputId": "aa16c877-9c71-4122-c8b7-d37f398b2ee5"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset_folder='./persian-license-plate-v1'  \n",
        "vocab_path='/content/vocab.json' \n",
        "trainP=os.path.join(dataset_folder,'data','train-00000-of-00001.parquet')\n",
        "valP=os.path.join(dataset_folder,'data','validation-00000-of-00001.parquet')\n",
        "testP=os.path.join(dataset_folder,'data','test-00000-of-00001.parquet')\n",
        "with open(vocab_path,'r',encoding='utf-8') as f:\n",
        "    vocab=json.load(f)\n",
        "\n",
        "char_map={\n",
        "    '۰': '0','۱': '1','۲': '2'\n",
        "    ,'۳': '3','۴': '4',\n",
        "    '۵': '5','۶': '6','۷': '7','۸': '8'\n",
        "    ,'۹': '9'\n",
        "}\n",
        "train_df=pd.read_parquet(trainP)\n",
        "val_df=pd.read_parquet(valP)\n",
        "test_df=pd.read_parquet(testP)\n",
        "\n",
        "def map_and_filter_labels(label):\n",
        "    mapped_label=''.join(char_map.get(char,char) for char in label)\n",
        "    return all(char in vocab for char in mapped_label),mapped_label\n",
        "\n",
        "for df,name in [(train_df,'train'),(val_df,'validation'),(test_df,'test')]:\n",
        "    valid_data=[]\n",
        "    for idx,row in tqdm(df.iterrows(),total=len(df),desc=f\"پردازش لیبل‌های {name}\"):\n",
        "        is_valid,mapped_label=map_and_filter_labels(row['label'])\n",
        "        if is_valid:\n",
        "            row=row.copy() \n",
        "            row['label']=mapped_label  \n",
        "            valid_data.append(row)\n",
        "    df=pd.DataFrame(valid_data)\n",
        "    if name=='train':\n",
        "        train_df=df\n",
        "    elif name == 'validation':\n",
        "        val_df=df\n",
        "    else:\n",
        "        test_df=df\n",
        "if len(train_df) == 0:\n",
        "    raise ValueError(\"vocab ERR\")\n",
        "if len(val_df) == 0:\n",
        "    print(\"No data\")\n",
        "if len(test_df) == 0:\n",
        "    print(\"NO data\")\n",
        "\n",
        "class PlateDataset(Dataset):\n",
        "    def __init__(self,df,transform=None):\n",
        "        self.df=df\n",
        "        self.transform=transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self,idx):\n",
        "        row=self.df.iloc[idx]\n",
        "        if 'image' in self.df.columns:\n",
        "            image_data=row['image']\n",
        "            if isinstance(image_data,bytes):\n",
        "                image=Image.open(io.BytesIO(image_data))\n",
        "            else:\n",
        "                image=Image.fromarray(image_data)\n",
        "        else:\n",
        "            raise ValueError(\"ERR\")\n",
        "        image=image.convert('L') \n",
        "        label=row['label']\n",
        "        label_encoded=[vocab[char] for char in label]\n",
        "        label_encoded=torch.tensor(label_encoded,dtype=torch.int64)\n",
        "\n",
        "        if self.transform:\n",
        "            image=self.transform(image)\n",
        "\n",
        "        return image,label_encoded\n",
        "\n",
        "train_dataset= PlateDataset(train_df,transform=train_transform)\n",
        "val_dataset=PlateDataset(val_df,transform=val_transform)\n",
        "test_dataset= PlateDataset(test_df,transform=val_transform)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    IMGE,lable=zip(*batch)\n",
        "    IMGE= torch.stack(IMGE)\n",
        "    return IMGE,lable\n",
        "\n",
        "trainL= DataLoader(train_dataset,batch_size=8,shuffle=True,num_workers=2,collate_fn=collate_fn)\n",
        "valL= DataLoader(val_dataset,batch_size=8,shuffle=False,num_workers=2,collate_fn=collate_fn)\n",
        "testL= DataLoader(test_dataset,batch_size=8,shuffle=False,num_workers=2,collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40BcKOk3QvrV",
        "outputId": "66a10993-7edf-4389-edd6-4bb6750f87ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['image_path', 'label'], dtype='object')\n",
            "                                          image_path     label\n",
            "0  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  ۱۴س۵۶۹۱۴\n",
            "1  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  ۶۶د۸۶۸۲۴\n",
            "2  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  ۴۶د۸۵۷۱۰\n",
            "3  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  ۷۲ص۳۹۸۱۰\n",
            "4  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  ۶۴س۸۵۴۸۸\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df=pd.read_parquet('./persian-license-plate-v1/data/train-00000-of-00001.parquet')\n",
        "print(df.columns)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size=8\n",
        "learning_rate=0.0002\n",
        "weight_decay=0.0001\n",
        "max_epochs= 200\n",
        "patience=20\n",
        "best_model_path=\"/content/drive/MyDrive/Fum/term9/Project_phase2/ocr_v3_p20.pth\"\n",
        "finetuned_model_path=\"/content/drive/MyDrive/Fum/term9/Project_phase2/ocr_finetuned.pth\"\n",
        "vocab_path='/content/vocab.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CRNN(nn.Module):\n",
        "    def __init__(self,num_classes,hidden_size=256):\n",
        "        super(CRNN,self).__init__()\n",
        "\n",
        "        self.cnn=nn.Sequential(\n",
        "            nn.Conv2d(1,64,kernel_size=3,padding=1),\n",
        "            PrintShape(\"Conv1\"),\n",
        "            nn.ReLU(True),\n",
        "            PrintShape(\"ReLU1\"),\n",
        "            nn.Dropout2d(0.3),\n",
        "            PrintShape(\"Dropout1\"),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "            PrintShape(\"MaxPool1\"),\n",
        "            nn.Conv2d(64,128,kernel_size=3,padding=1),\n",
        "            PrintShape(\"Conv2\"),\n",
        "            nn.ReLU(True),\n",
        "            PrintShape(\"ReLU2\"),\n",
        "            nn.Dropout2d(0.3),\n",
        "            PrintShape(\"Dropout2\"),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "            PrintShape(\"MaxPool2\"),\n",
        "            nn.Conv2d(128,256,kernel_size=3,padding=1),\n",
        "            PrintShape(\"Conv3\"),\n",
        "            nn.ReLU(True),\n",
        "            PrintShape(\"ReLU3\"),\n",
        "            nn.Conv2d(256,256,kernel_size=3,padding=1),\n",
        "            PrintShape(\"Conv4\"),\n",
        "            nn.ReLU(True),\n",
        "            PrintShape(\"ReLU4\"),\n",
        "            nn.Dropout2d(0.3),\n",
        "            PrintShape(\"Dropout3\"),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "            PrintShape(\"MaxPool3\"),\n",
        "            nn.Conv2d(256,512,kernel_size=3,padding=1),\n",
        "            PrintShape(\"Conv5\"),\n",
        "            nn.ReLU(True),\n",
        "            PrintShape(\"ReLU5\"),\n",
        "            nn.BatchNorm2d(512),\n",
        "            PrintShape(\"BatchNorm\"),\n",
        "            nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
        "            PrintShape(\"Conv6\"),\n",
        "            nn.ReLU(True),\n",
        "            PrintShape(\"ReLU6\"),\n",
        "            nn.Dropout2d(0.3),\n",
        "            PrintShape(\"Dropout4\"),\n",
        "            nn.MaxPool2d(kernel_size=(4,1)),\n",
        "            PrintShape(\"MaxPool4\"),\n",
        "            nn.Conv2d(512,512,kernel_size=2,stride=1),\n",
        "            PrintShape(\"Conv7\"),\n",
        "            nn.ReLU(True),\n",
        "            PrintShape(\"ReLU7\")\n",
        "        )\n",
        "\n",
        "        self.dropout_rnn=nn.Dropout(0.2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.rnn=nn.Sequential(\n",
        "            nn.LSTM(input_size=512,hidden_size=hidden_size,num_layers=2,\n",
        "                    bidirectional=True,batch_first=True)\n",
        "        )\n",
        "\n",
        "        self.fc=nn.Linear(hidden_size * 2,num_classes)\n",
        "\n",
        "    def forward(self,x):\n",
        "        conv=self.cnn(x)\n",
        "        batch_size,channels,height,width=conv.size()\n",
        "        conv=conv.squeeze(2)\n",
        "        conv=conv.permute(0,2,1)\n",
        "        conv=self.dropout_rnn(conv)\n",
        "        rnn_out,_=self.rnn(conv)\n",
        "        out=self.fc(rnn_out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_epoch(M,loader,criterion):\n",
        "    M.eval()\n",
        "    total_loss=0\n",
        "    with torch.no_grad():\n",
        "        for IMGE,lable in tqdm(loader,desc=\"ارزیابی\"):\n",
        "            IMGE=IMGE.to(device)\n",
        "            max_len=max([len(label) for label in lable])\n",
        "            padded_labels=torch.zeros(len(lable),max_len,dtype=torch.long).to(device)\n",
        "            for i,label in enumerate(lable):\n",
        "                padded_labels[i,:len(label)]=label\n",
        "            outputs=M(IMGE).log_softmax(2)\n",
        "\n",
        "            \n",
        "            batch_size=IMGE.size(0)\n",
        "            input_lengths=torch.full((batch_size,),outputs.size(1),dtype=torch.long)\n",
        "            target_lengths=torch.tensor([len(label) for label in lable],dtype=torch.long)\n",
        "            loss=criterion(outputs.permute(1,0,2),padded_labels,input_lengths,target_lengths)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_accuracy(M,loader,reverse_vocab):\n",
        "    M.eval()\n",
        "    total_correctP=0\n",
        "    total_correctC=0\n",
        "    totalC= 0\n",
        "    totalS= 0\n",
        "    with torch.no_grad():\n",
        "        for IMGE,lable in loader:\n",
        "            IMGE= IMGE.to(device)\n",
        "            outputs= M(IMGE).softmax(2)\n",
        "            decodedP=decode_predictions(outputs,reverse_vocab)\n",
        "            for i in range(len(lable)):\n",
        "                label=lable[i].cpu().numpy()\n",
        "                gt= ''.join([reverse_vocab[idx] for idx in label if idx in reverse_vocab])\n",
        "                pred= decodedP[i]\n",
        "                if pred==gt:\n",
        "                    total_correctP += 1\n",
        "                min_len= min(len(pred),len(gt))\n",
        "\n",
        "                \n",
        "                correct_chars= sum(1 for p,g in zip(pred[:min_len],gt[:min_len]) if p == g)\n",
        "                total_correctC += correct_chars\n",
        "                totalC+= len(gt)\n",
        "                totalS += 1\n",
        "    plate_accuracy= (total_correctP / totalS) * 100\n",
        "    char_accuracy= (total_correctC / totalC) * 100\n",
        "    return plate_accuracy,char_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PlateDataset(Dataset):\n",
        "    def __init__(self,df,transform=None):\n",
        "        self.df=df\n",
        "        self.transform=transform\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self,idx):\n",
        "        row=self.df.iloc[idx]\n",
        "\n",
        "\n",
        "        image_data=row['image_path']['bytes']\n",
        "        \n",
        "        image=Image.open(io.BytesIO(image_data))\n",
        "        image=image.convert('L')\n",
        "        label=row['label']\n",
        "        label_encoded=[vocab[char] for char in label]\n",
        "        label_encoded=torch.tensor(label_encoded,dtype=torch.int64)\n",
        "        if self.transform:\n",
        "            image=self.transform(image)\n",
        "        return image,label_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_folder='./persian-license-plate-v1'\n",
        "trainP=os.path.join(dataset_folder,'data','train-00000-of-00001.parquet')\n",
        "valP=os.path.join(dataset_folder,'data','validation-00000-of-00001.parquet')\n",
        "testP=os.path.join(dataset_folder,'data','test-00000-of-00001.parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"لود وکبیولری...\")\n",
        "with open(vocab_path,'r',encoding='utf-8') as f:\n",
        "    vocab=json.load(f)\n",
        "reverse_vocab={v: k for k,v in vocab.items()}\n",
        "char_map={\n",
        "    '۰': '0','۱': '1','۲': '2'\n",
        "    ,'۳': '3','۴': '4',\n",
        "    '۵': '5','۶': '6','۷': '7'\n",
        "    ,'۸': '8','۹': '9',\n",
        "    'ي': 'ی'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_transform=transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((64,128)),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2),\n",
        "    transforms.GaussianBlur(kernel_size=5,sigma=0.5),\n",
        "    transforms.RandomAffine(degrees=0,shear=5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5],std=[0.5])\n",
        "])\n",
        "val_transform=transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((64,128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5],std=[0.5])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndaGkTy2B-P7",
        "outputId": "57c11642-41f7-4634-e123-080802313a0a"
      },
      "outputs": [],
      "source": [
        "train_df=pd.read_parquet(trainP)\n",
        "val_df=pd.read_parquet(valP)\n",
        "test_df=pd.read_parquet(testP)\n",
        "def map_and_filter_labels(label) :\n",
        "    mapped_label=''.join(char_map.get(char,char) for char in label)\n",
        "    return all(char in vocab for char in mapped_label),mapped_label\n",
        "for df,name in [(train_df,'train'),(val_df,'validation'),(test_df,'test')]:\n",
        "    valid_data=[]\n",
        "    for idx,row in tqdm(df.iterrows() ,total=len(df),desc=f\"پردازش لیبل‌های {name}\"):\n",
        "        is_valid,mapped_label=map_and_filter_labels(row['label'])\n",
        "        if is_valid:\n",
        "            row=row.copy()\n",
        "            row['label']=mapped_label\n",
        "            valid_data.append(row)\n",
        "        else:\n",
        "            print(f\"DEL {row['label']} -> {mapped_label}\")\n",
        "    df=pd.DataFrame(valid_data)\n",
        "    if name == 'train':\n",
        "        train_df= df\n",
        "    elif name== 'validation':\n",
        "        val_df= df\n",
        "    else:\n",
        "        test_df= df\n",
        "\n",
        "\n",
        "\n",
        "train_dataset=PlateDataset(train_df,transform=train_transform)\n",
        "val_dataset=PlateDataset(val_df,transform=val_transform)\n",
        "test_dataset=PlateDataset(test_df,transform=val_transform)\n",
        "def collate_fn(batch):\n",
        "    IMGE,lable=zip(*batch)\n",
        "    IMGE=torch.stack(IMGE)\n",
        "    return IMGE,lable\n",
        "\n",
        "trainL=DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=2,collate_fn=collate_fn)\n",
        "valL=DataLoader(val_dataset,batch_size=batch_size,shuffle=False,num_workers=2,collate_fn=collate_fn)\n",
        "testL=DataLoader(test_dataset,batch_size=batch_size,shuffle=False,num_workers=2,collate_fn=collate_fn)\n",
        "\n",
        "class PrintShape(nn.Module):\n",
        "    def __init__(self,layer_name):\n",
        "        super(PrintShape,self).__init__()\n",
        "        self.layer_name=layer_name\n",
        "    def forward(self,x):\n",
        "        return x\n",
        "num_classes=len(vocab)   \n",
        "M=CRNN(num_classes,hidden_size=256).to(device)\n",
        "print(f\"Load {best_model_path}\")\n",
        "M.load_state_dict(torch.load(best_model_path))\n",
        "M.to(device)\n",
        "criterion=nn.CTCLoss(blank=0,zero_infinity=True)\n",
        "optimizer=optim.Adam(M.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
        "scheduler=optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',factor=0.5,patience=5)\n",
        "def decode_predictions(outputs,reverse_vocab):\n",
        "    _,max_indices=torch.max(outputs,dim=2)\n",
        "    decoded=[]\n",
        "    for seq in max_indices:\n",
        "        prev=None\n",
        "        decoded_seq=[]\n",
        "        for idx in seq:\n",
        "            idx=idx.item()\n",
        "            if idx != 0 and idx != prev:  \n",
        "                if idx in reverse_vocab:\n",
        "                    decoded_seq.append(reverse_vocab[idx])\n",
        "            prev=idx\n",
        "        decoded.append(''.join(decoded_seq))\n",
        "    return decoded\n",
        "def train_epoch(M,loader,criterion,optimizer):\n",
        "    M.train()\n",
        "    total_loss=0\n",
        "    for IMGE,lable in tqdm(loader,desc=\"آموزش\"):\n",
        "        IMGE=IMGE.to(device)\n",
        "        max_len=max([len(label) for label in lable])\n",
        "        padded_labels=torch.zeros(len(lable),max_len,dtype=torch.long).to(device)\n",
        "        for i,label in enumerate(lable):\n",
        "            padded_labels[i,:len(label)]=label\n",
        "        optimizer.zero_grad()\n",
        "        outputs=M(IMGE).log_softmax(2)\n",
        "        batch_size=IMGE.size(0)\n",
        "        input_lengths=torch.full((batch_size,),outputs.size(1),dtype=torch.long)\n",
        "        target_lengths=torch.tensor([len(label) for label in lable],dtype=torch.long)\n",
        "        loss=criterion(outputs.permute(1,0,2),padded_labels,input_lengths,target_lengths)\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            print(f\"invalid loss: {loss.item()}\")\n",
        "            continue\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(M.parameters(),max_norm=5.0)\n",
        "        optimizer.step()\n",
        "    return total_loss / len(loader)\n",
        "best_val_loss=float('inf')\n",
        "patienceC=0\n",
        "for epoch in range(max_epochs):\n",
        "    train_loss=train_epoch(M,trainL,criterion,optimizer)\n",
        "    val_loss=validate_epoch(M,valL,criterion)\n",
        "    train_plate_acc,train_char_acc=compute_accuracy(M,trainL,reverse_vocab)\n",
        "    val_plate_acc,val_char_acc=compute_accuracy(M,valL,reverse_vocab)\n",
        "    print(f\"Epoch {epoch+1}/{max_epochs}:\")\n",
        "    print(f\"Train Loss: {train_loss:.4f},Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"Train Plate Acc: {train_plate_acc:.2f}%,Train Char Acc: {train_char_acc:.2f}%\")\n",
        "    print(f\"Val Plate Acc: {val_plate_acc:.2f}%,Val Char Acc: {val_char_acc:.2f}%\")\n",
        "    scheduler.step(val_loss)\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss=val_loss\n",
        "        patienceC=0\n",
        "        torch.save(M.state_dict(),finetuned_model_path)\n",
        "        print(f\"SAVE: {finetuned_model_path}\")\n",
        "    else:\n",
        "        patienceC += 1\n",
        "        if patienceC>=patience:\n",
        "            print(\"Early Stopping!\")\n",
        "            break\n",
        "test_plate_acc,test_char_acc=compute_accuracy(M,testL,reverse_vocab)\n",
        "print(f\"Test Plate Accuracy: {test_plate_acc:.2f}%\")\n",
        "print(f\"Test Character Accuracy: {test_char_acc:.2f}%\")\n",
        "print(\"fine-tuning END!!!!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
